# financial-sentiment-analysis
üì∞ Financial Sentiment Analysis in News Headlines

Team Members:
	‚Ä¢	K2453494 ‚Äì Adharsh Vaiapuri
	‚Ä¢	K2501085 ‚Äì Avez Mushtaq Kazi

‚∏ª

üéØ Project Overview

Financial markets are highly sensitive to news. Even subtle wording in financial headlines can influence investor sentiment and, in turn, market behavior.

This project applies Natural Language Processing (NLP) techniques and transformer-based deep learning models to automatically classify financial news headlines into positive, neutral, or negative sentiments.

The goal is to build an efficient and accurate sentiment classification model that can assist in tasks like market trend analysis, risk assessment, and automated trading insights.

‚∏ª

üß† Problem Statement

Traditional sentiment models struggle with financial jargon and subtle tone variations.
This project aims to fine-tune and compare two state-of-the-art transformer models:
	‚Ä¢	BERT (Bidirectional Encoder Representations from Transformers)
	‚Ä¢	DistilBERT (a smaller, faster distilled version of BERT)

Both models are trained using the Hugging Face transformers library and evaluated on labelled financial news data.

‚∏ª

üìä Dataset
	‚Ä¢	Size: ~10,000 financial news headlines
	‚Ä¢	Split: 70% Train | 15% Validation | 15% Test
	‚Ä¢	Labels:
	‚Ä¢	0 ‚Üí Negative
	‚Ä¢	1 ‚Üí Neutral
	‚Ä¢	2 ‚Üí Positive

Exploratory Data Analysis (EDA):
	‚Ä¢	Label distribution via countplot and pie chart
	‚Ä¢	Word clouds for each sentiment class
	‚Ä¢	Sentence length distribution
	‚Ä¢	Visualization of sample text vs. sentiment label

‚∏ª

‚öôÔ∏è Methodology

üß© 1. Preprocessing
	‚Ä¢	Lowercasing
	‚Ä¢	Removal of special characters & extra spaces
	‚Ä¢	Uniform label encoding
	‚Ä¢	Tokenization with AutoTokenizer

üîç 2. Model Fine-Tuning

BERT (bert-base-uncased)
	‚Ä¢	Model: BertForSequenceClassification
	‚Ä¢	Batch size: 4 (train), 8 (eval)
	‚Ä¢	Epochs: 1
	‚Ä¢	Optimizer: AdamW

DistilBERT
	‚Ä¢	Model: DistilBertForSequenceClassification
	‚Ä¢	Batch size: 8 (train), 16 (eval)
	‚Ä¢	Epochs: 3
	‚Ä¢	Optimizer: AdamW

Both trained using the Hugging Face Trainer API with a custom PyTorch Dataset class.


üìà Results & Analysis
Model
Training Time
Accuracy
Parameters
BERT
~40 min
96.0%
110M
DistilBERT
~15 min
96.0%
66M



Key Insights:
	‚Ä¢	BERT offers stronger contextual understanding and slightly better precision.
	‚Ä¢	DistilBERT achieves comparable accuracy with 2.5√ó faster training and less GPU memory usage.
	‚Ä¢	Both models effectively capture nuanced financial tone.

Confusion Matrix Analysis:
	‚Ä¢	BERT performs better at separating neutral vs. positive sentiment.
	‚Ä¢	DistilBERT shows minor confusion between neutral and negative headlines.


üßæ Implementation Notes
	‚Ä¢	Frameworks: PyTorch, Hugging Face Transformers
	‚Ä¢	Visualization: Matplotlib, Seaborn, WordCloud
	‚Ä¢	GPU Tested: NVIDIA T4 / A100
	‚Ä¢	Modular and documented notebook with debugging checkpoints

‚∏ª

üß© Key Takeaways
	‚Ä¢	BERT: Best for high-stakes financial sentiment tasks where accuracy matters most.
	‚Ä¢	DistilBERT: Excellent balance of speed and performance ‚Äî ideal for real-time or resource-limited systems.
	‚Ä¢	Demonstrates how transformer models outperform classical NLP in domain-specific sentiment tasks.
  
üèÅ Conclusion

Both BERT and DistilBERT proved highly effective for financial sentiment analysis, achieving 96% accuracy.
While BERT excels in contextual depth, DistilBERT is a practical alternative for faster, resource-efficient deployment.

This project demonstrates how transformer-based NLP models can empower intelligent financial decision systems through text sentiment analysis.
